{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "#import mpl_toolkits\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Nursultan\\Documents\\Data Science\\breast cancer\\wdbc.data.csv'\n",
    "df = pd.read_csv(path, names = ['ID', 'Diagnosis',\n",
    "                    'MRadius', 'MTexture', 'Mper-ter', 'Marea', 'MSmooth-es',\n",
    "                    'MComp-ness', 'MConcavity', 'MConcavPoints', 'MSymm-ry',\n",
    "                    'MFractDimens', 'RadiusSE', 'TextureSE', 'Per-terSE',\n",
    "                    'AreaSE', 'Smooth-esSE',\n",
    "                    'Comp-nessSE', 'ConcavitySE', 'ConcavPointsSE', \n",
    "                    'Symm-rySE', 'FractDimensSE', 'WRadius', 'WTexture',\n",
    "                    'WPer-ter', 'WArea', 'WSmooth-es',\n",
    "                    'WComp-ness', 'WConcavity', 'WConcavPoints', \n",
    "                    'WSymm-ry', 'WFractDimens'])\n",
    "df['Malignant'] = df.Diagnosis.map({'B':0, 'M':1})\n",
    "df = df.drop('Diagnosis', 1)\n",
    "df = df.drop('ID', 1)\n",
    "df['Diagnosis'] = df['Malignant'].copy()\n",
    "df['Diagnosis'].replace(0, 'Benign',inplace=True)\n",
    "df['Diagnosis'].replace(1, 'Malignant',inplace=True)\n",
    "#df.iloc[:5, 29]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X, y = df.iloc[:, :30].values, df.iloc[:, 30].values\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling \n",
    "We can see the features are measured on different scales, and would be beneficial\n",
    "for almost any classifier. It's gonna be useful for my gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Standardizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn import preprocessing\n",
    "print(df.iloc[:4,3])\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit( df.iloc[:,2:32].values)\n",
    "df_std = std_scale.transform( df.iloc[:,2:32].values)\n",
    "print('Mean after standardization:')\n",
    "print('   Mean Radius={:.2f}, Mean Area={:.2f}'.format(df_std[:,2].mean(), df_std[:,5].mean()) ,\\\n",
    "      'Mean Smoothness={:.2f} '.format(df_std[:,6].mean()))\n",
    "print('Stand. dev. after stand-tion:')\n",
    "print('   Mean Radius={:.2f}, Mean Area={:.2f},'.format(df_std[:,2].std(), df_std[:,5].std()) ,\\\n",
    "      'Mean Smoothness={:.2f} '.format(df_std[:,6].std()))\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigendecomposition of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eigh(cov_mat)\n",
    "\n",
    "print('\\nEigenvalues \\n%s' % eigen_vals)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total and explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "plt.bar(range(1, 31), var_exp, alpha=0.5, align='center',\n",
    "        label='individual variance explained variance')\n",
    "plt.step(range(1, 31), cum_var_exp, where='mid',\n",
    "        label='cumulative variance ratio')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(r'C:\\Users\\Nursultan\\Documents\\Data Science\\breast cancer\\figures\\PCA1.png', dpi=300)\n",
    "plt.tight_layout()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "#Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(reverse=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "              eigen_pairs[1][1][:, np.newaxis],\n",
    "              #eigen_pairs[2][1][:, np.newaxis]\n",
    "              ))\n",
    "w\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "'''\n",
    "X_train_pca = X_train_std.dot(w)\n",
    "colors = ['r', 'g']\n",
    "markers = ['x', 'o']\n",
    "\n",
    "for l,c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0],\n",
    "                X_train_pca[y_train==l, 1],\n",
    "                c=c, label=c, marker=m)\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'C:\\Users\\Nursultan\\Documents\\Data Science\\breast cancer\\figures\\PCA2.png', dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train_std[0].dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised data compression via linear discriminant analysis\n",
    "        It's a always useful LDA is a dimensionality reduction technique, similar to PCA. The\" goal is to\n",
    "    project a dataset onto a lower-dimensional space with good class-separability in order to avoid overfitting (\"curse of dimensionality\") and also reduce computational costs.\" ( From Sebastian Raschka's article) http://sebastianraschka.com/Articles/2014_python_lda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing scatter matrices\n",
    "    Calculate the mean vector for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "mean_vecs = []\n",
    "for label in range(0,2):\n",
    "    mean_vecs.append(np.mean(X_train_std[y_train==label], axis=0))\n",
    "    print('MV %s: %s\\n' %(label, mean_vecs[label-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the within-class scatter-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 30\n",
    "S_W = np.zeros((n_features, n_features))\n",
    "for label, mv in zip(range(0,2), mean_vecs):\n",
    "    class_scatter = np.zeros((n_features, n_features))\n",
    "    for row in X_train_std[y_train == label]:\n",
    "        row, mv = row.reshape(n_features, 1), mv.reshape(n_features, 1)\n",
    "        class_scatter += (row-mv).dot((row-mv).T)\n",
    "    S_W += class_scatter\n",
    "print('Within-class scatter matrix: %sx%s ' % (S_W.shape[0], S_W.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Class label distribution: %s' % np.bincount(y_train)[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 30\n",
    "S_W = np.zeros((n_features, n_features))\n",
    "for label,mv in zip(range(0, 2), mean_vecs):\n",
    "    class_scatter = np.cov(X_train_std[y_train==label].T)\n",
    "    S_W += class_scatter\n",
    "print('Scaled within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the between-class scatter matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_overall = np.mean(X_train_std, axis=0)\n",
    "n_features = 30\n",
    "S_B = np.zeros((n_features, n_features))\n",
    "for i,mean_vec in enumerate(mean_vecs):\n",
    "    n = X_train[y_train==i+1, :].shape[0]\n",
    "    mean_vec = mean_vec.reshape(n_features, 1) # make column vector\n",
    "    mean_overall = mean_overall.reshape(n_features, 1) # make column vector\n",
    "    S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\n",
    "\n",
    "print('Between-class scatter matrix: %sx%s' % (S_B.shape[0], S_B.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting linear discriminants for the new feature subspace\n",
    "We are interested in the eigenvectors that correspond to largest eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "#Sorting eigenvectors in decreasing order of the eigenvalues:\n",
    "# Making a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "#Soring tuples from high to low\n",
    "eigen_pairs = sorted(eigen_pairs, key = lambda k : k[0], reverse =True)\n",
    "\n",
    "print('Eigenvalues in decreasing order:\\n')\n",
    "for eigen_val in eigen_pairs:\n",
    "    print(eigen_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals.real)\n",
    "discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\n",
    "cum_discr = np.cumsum(discr)\n",
    "\n",
    "plt.bar(range(1, 31), discr, alpha=0.5, align='center',\n",
    "       label='individual \"discriminability\" ')\n",
    "plt.step(range(1, 31), cum_discr, where='mid',\n",
    "        label='cumulative \"discriminability\"')\n",
    "plt.ylabel('\"discriminability ration')\n",
    "plt.xlabel('Linear Discriminants')\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n",
    "                      eigen_pairs[1][1][:, np.newaxis].real))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecting onto the new feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_lda = X_train_std.dot(w)\n",
    "colors = ['r', 'g']\n",
    "markers = ['x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_lda[y_train==l, 0] * (-1), \n",
    "                X_train_lda[y_train==l, 1] * (-1), \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA via scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, y_train)\n",
    "X_train_lda.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    markers = ('x', 'o', 's', '^')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                         np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=cmap(idx),\n",
    "                    marker=markers[idx], label=cl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "#lr = LogisticRegression()\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X_train_lda, y_train)\n",
    "X_test_lda = lda.transform(X_test_std)\n",
    "y_pred = gnb.predict(X_test_lda)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Feedforward Neural Network\n",
    "    Setting the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = len(X_train_lda)\n",
    "print(num_samples)\n",
    "nn_inputs = 1\n",
    "nn_outputs = 2\n",
    "epsilon = 0.01\n",
    "reg_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_error(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    #z1 = X.dot(W1) + b1\n",
    "    z1 = X_train_lda.dot(W1) + b1\n",
    "    #a1= np.tanh(z1)\n",
    "    #using sigmoid activation function instead of hyperbolic tangent\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims = True)\n",
    "    \n",
    "    #calculating the error\n",
    "    #correct_logprobs = - np.log(probs[range(num_samples), y])\n",
    "    correct_logprobs = - np.log(probs[range(num_samples), y_train])\n",
    "    error = np.sum(correct_logprobs)\n",
    "    #optional regularization term\n",
    "    error += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_samples * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    #propagate forward\n",
    "    #calc the dot product of input layer and its weights and add 1st bias term\n",
    "    z1 = x.dot(W1) + b1\n",
    "    #apply tanh activation function\n",
    "    #a1 = np.tanh(z1)\n",
    "    #using sigmoid activation function instead of hyperbolic tangent\n",
    "    a1 = sigmoid(z1)\n",
    "    #calc the dot product of the output of tanh func applied\n",
    "    # to input layer and its weights and add 2nd bias term\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores/np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to learn the param-s for the NN and return the model\n",
    "# hid_dim - hidden layer dimension - # of nodes in hidden layer\n",
    "# num_passes - num of passes through the training set for gradient descent\n",
    "# print_loss - if true, prints the loss every 1000 iterations\n",
    "def build_model(hid_dim, num_passes = 5000, print_loss=False):\n",
    "    #initialize param-s randomly, we need to learn them\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_inputs, hid_dim)/ np.sqrt(nn_inputs)\n",
    "    b1 = np.zeros((1, hid_dim))\n",
    "    W2 = np.random.randn(hid_dim,nn_outputs) / np.sqrt(hid_dim)\n",
    "    b2 = np.zeros((1, nn_outputs))\n",
    "    \n",
    "    model = {}\n",
    "    \n",
    "    #Gradient descent\n",
    "    for i in xrange(0, num_passes):\n",
    "        \n",
    "        #feed forward\n",
    "        #z1 = X.dot(W1) + b1\n",
    "        z1 = X_train_lda.dot(W1) + b1\n",
    "        #a1 = np.tanh(z1)\n",
    "        #using sigmoid activation function instead of hyperbolic tangent\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        #Backpropagate\n",
    "        delta3 = probs\n",
    "        # delta3[range(num_samples), y] -= 1\n",
    "        delta3[range(num_samples), y_train] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        #delta2 = delta3.dot(W2.T) * (1 - np.power(a1,2))\n",
    "        #derivative of sigmoid function f(x) = 1./ (1+ e^(-x)) is f'(x) = f(x) * (1-f(x))\n",
    "        delta2 = delta3.dot(W2.T) * (a1 * (1 - a1))\n",
    "        #dW1 = np.dot(X.T, delta2)\n",
    "        dW1 = np.dot( X_train_lda.T, delta2)\n",
    "        db1 = np.sum(delta2, axis =0)\n",
    "        \n",
    "        #add regularization terms (b1 and b2 aren't regularized)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "        \n",
    "        #Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        #optionally print the loss\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print \"Loss after iteration %i: %f\" %(i, calc_error(model))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a model with a 1-dim hidden layer\n",
    "model = build_model(1, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "minmax_scale = preprocessing.MinMaxScaler().fit(df.iloc[:,:30].values)\n",
    "df_minmax = minmax_scale.transform(df.iloc[:,:30].values)\n",
    "print('Min value after min-max:')\n",
    "print('  Mean Radius={:.2f}, Mean Area={:.2f}'.format(df_minmax[:,0].min(), df_minmax[:,3].min()) ,\\\n",
    "      'Mean Smoothness={:.2f}'.format(df_minmax[:,4].min())) \n",
    "print('Max value after min-max:')\n",
    "print('  Mean Radius={:.2f}, Mean Area={:.2f}'.format(df_minmax[:,0].max(), df_minmax[:,3].max()) ,\\\n",
    "      'Mean Smoothness={:.2f}'.format(df_minmax[:,4].max())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot():\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    plt.scatter(df.iloc[:,0], df.iloc[:,4], \n",
    "               color = 'green', label='input scale', alpha=0.5)\n",
    "    plt.scatter(X_train_std[:,0], X_train_std[:,4], color='red',\n",
    "            label = 'Standardized', alpha=0.3)\n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,4], color='blue', \n",
    "                label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "    plt.title(\" Mean Radius and Mean Smoothness content\")\n",
    "    plt.xlabel('Radius')\n",
    "    plt.ylabel('Smoothness')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plot()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "malignant = df[df['Malignant'].isin([1])]\n",
    "benign = df[df['Malignant'].isin([0])]\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(malignant['MTexture'], malignant['WArea'], s=50, marker='x', label='Malignant')\n",
    "ax.scatter(benign['MTexture'], benign['WArea'], s=50, marker='o', label='Benign')\n",
    "ax.legend()\n",
    "\n",
    "seabornMatrix = sns.pairplot(df[['Malignant','MTexture','WArea','WSmooth-es']],hue=\"Malignant\")\n",
    "seabornMatrix.savefig(\"SeabornMatrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.index=range(0,569,1)\n",
    "visualIn3D = plt.figure().gca(projection='3d')\n",
    "fig3D = plt.gcf()\n",
    "fig3D.set_size_inches(15.5, 10.5)\n",
    "visualIn3D.scatter(df['MTexture'], df['WArea'], df['WSmooth-es'], c= 'b')\n",
    "visualIn3D.set_xlabel('Mean Texture')\n",
    "visualIn3D.set_ylabel('Worst Area')\n",
    "visualIn3D.set_zlabel('Worst Smoothness')\n",
    "plt.show()\n",
    "fig3D.savefig('3D visualization.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
